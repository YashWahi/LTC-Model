# LTC Control with Deep Reinforcement Learning
# File: ltc_drl_controller.py

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from collections import deque
import random
import os
import zipfile
import matplotlib.pyplot as plt

# 1. Data Preparation and Environment Setup
class LTCEnvironment:
    def __init__(self, data_path):
        # Extract and load dataset
        with zipfile.ZipFile(data_path, 'r') as zip_ref:
            zip_ref.extractall('ltc_data')
        
        # Load voltage and load data (assuming CSV format)
        self.voltage_data = pd.read_csv('ltc_data/voltage_records.csv')
        self.load_data = pd.read_csv('ltc_data/load_records.csv')
        
        # Environment parameters
        self.min_voltage = 0.95  # pu
        self.max_voltage = 1.05   # pu
        self.tap_step = 0.0125    # per tap change
        self.max_taps = 16        # Â±8 taps
        self.current_tap = 0
        self.current_step = 0
        self.max_steps = len(self.voltage_data) - 1
        
    def reset(self):
        """Reset environment to initial state"""
        self.current_tap = 0
        self.current_step = 0
        return self._get_state()
    
    def _get_state(self):
        """Get current state observation"""
        return np.array([
            self.voltage_data.iloc[self.current_step]['voltage'],
            self.load_data.iloc[self.current_step]['load'],
            self.current_tap
        ])
    
    def step(self, action):
        """
        Execute action (0: decrease tap, 1: hold, 2: increase tap)
        Returns: next_state, reward, done, info
        """
        # Apply action
        if action == 0 and self.current_tap > -self.max_taps//2:
            self.current_tap -= 1
        elif action == 2 and self.current_tap < self.max_taps//2:
            self.current_tap += 1
        
        # Calculate new voltage after tap change
        voltage_change = self.current_tap * self.tap_step
        current_voltage = self.voltage_data.iloc[self.current_step]['voltage'] + voltage_change
        
        # Calculate reward
        voltage_deviation = max(
            abs(current_voltage - 1.0) - 0.02,  # 2% deadband
            0
        )
        reward = -voltage_deviation - 0.01*abs(action-1)  # Penalize unnecessary tap changes
        
        # Move to next time step
        self.current_step += 1
        done = self.current_step >= self.max_steps
        
        return self._get_state(), reward, done, {'voltage': current_voltage}

# 2. DRL Agent Implementation
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95    # discount rate
        self.epsilon = 1.0    # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()
        
    def _build_model(self):
        """Build DQN model"""
        model = keras.Sequential([
            layers.Dense(64, input_dim=self.state_size, activation='relu'),
            layers.Dense(64, activation='relu'),
            layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.learning_rate))
        return model
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
    
    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        states = np.array([i[0] for i in minibatch])
        actions = np.array([i[1] for i in minibatch])
        rewards = np.array([i[2] for i in minibatch])
        next_states = np.array([i[3] for i in minibatch])
        dones = np.array([i[4] for i in minibatch])
        
        targets = self.model.predict(states)
        next_q_values = self.model.predict(next_states)
        
        for i in range(len(minibatch)):
            if dones[i]:
                targets[i][actions[i]] = rewards[i]
            else:
                targets[i][actions[i]] = rewards[i] + self.gamma * np.amax(next_q_values[i])
        
        self.model.fit(states, targets, epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save(self, name):
        self.model.save_weights(name)
    
    def load(self, name):
        self.model.load_weights(name)

# 3. Training Process
def train_agent(episodes=100, batch_size=32):
    env = LTCEnvironment('dt health.zip')
    state_size = env._get_state().shape[0]
    action_size = 3  # decrease, hold, increase
    
    agent = DQNAgent(state_size, action_size)
    done = False
    rewards_history = []
    voltage_history = []
    
    for e in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        total_reward = 0
        voltage_trace = []
        
        while not done:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            next_state = np.reshape(next_state, [1, state_size])
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            voltage_trace.append(info['voltage'])
            
            if len(agent.memory) > batch_size:
                agent.replay(batch_size)
        
        rewards_history.append(total_reward)
        voltage_history.append(voltage_trace)
        print(f"Episode: {e+1}/{episodes}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}")
        done = False
    
    # Save trained model
    agent.save('ltc_drl_model.h5')
    
    # Plot results
    plot_results(rewards_history, voltage_history[-1])
    
    return agent

def plot_results(rewards, voltages):
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(rewards)
    plt.title('Training Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(voltages)
    plt.axhline(y=1.05, color='r', linestyle='--')
    plt.axhline(y=0.95, color='r', linestyle='--')
    plt.title('Controlled Voltage')
    plt.xlabel('Time Step')
    plt.ylabel('Voltage (pu)')
    
    plt.tight_layout()
    plt.savefig('training_results.png')
    plt.close()

# 4. Evaluation Function
def evaluate_agent(agent, env):
    state = env.reset()
    state = np.reshape(state, [1, env._get_state().shape[0]])
    done = False
    voltage_trace = []
    tap_changes = []
    
    while not done:
        action = agent.act(state)
        next_state, _, done, info = env.step(action)
        next_state = np.reshape(next_state, [1, env._get_state().shape[0]])
        state = next_state
        voltage_trace.append(info['voltage'])
        tap_changes.append(action)
    
    # Plot evaluation results
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(voltage_trace)
    plt.axhline(y=1.05, color='r', linestyle='--')
    plt.axhline(y=0.95, color='r', linestyle='--')
    plt.title('Voltage Regulation')
    plt.xlabel('Time Step')
    plt.ylabel('Voltage (pu)')
    
    plt.subplot(1, 2, 2)
    plt.step(range(len(tap_changes)), tap_changes, where='post')
    plt.yticks([0, 1, 2], ['Decrease', 'Hold', 'Increase'])
    plt.title('Tap Change Actions')
    plt.xlabel('Time Step')
    plt.ylabel('Action')
    
    plt.tight_layout()
    plt.savefig('evaluation_results.png')
    plt.close()

# Main Execution
if __name__ == "__main__":
    # Train the agent
    trained_agent = train_agent(episodes=50)
    
    # Evaluate the trained agent
    env = LTCEnvironment('dt health.zip')
    evaluate_agent(trained_agent, env)
